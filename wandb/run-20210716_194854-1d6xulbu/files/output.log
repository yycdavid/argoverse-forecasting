Training begins ...
2021-07-16 19:49:12.745821: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-07-16 19:49:12.746787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:86:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-07-16 19:49:12.746938: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jasonyma/.mujoco/mujoco200/bin:/home/jasonyma/.mujoco/mujoco200/bin
2021-07-16 19:49:12.749347: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-07-16 19:49:12.751368: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-07-16 19:49:12.751683: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-07-16 19:49:12.754258: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-07-16 19:49:12.756228: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-07-16 19:49:12.756409: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jasonyma/.mujoco/mujoco200/bin:/home/jasonyma/.mujoco/mujoco200/bin
2021-07-16 19:49:12.756421: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-07-16 19:49:12.757045: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-07-16 19:49:12.803516: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2400000000 Hz
2021-07-16 19:49:12.811904: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5648de292e50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-07-16 19:49:12.811948: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-07-16 19:49:12.812093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-07-16 19:49:12.812111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
Train -- Epoch:0, loss:19.985445022583008, Rollout:30
Train -- Epoch:0, loss:16.89398765563965, Rollout:30
Train -- Epoch:0, loss:16.620092391967773, Rollout:30
Train -- Epoch:0, loss:17.793376922607422, Rollout:30
Train -- Epoch:0, loss:16.654382705688477, Rollout:30
Train -- Epoch:0, loss:20.245620727539062, Rollout:30
Train -- Epoch:0, loss:16.790164947509766, Rollout:30
Train -- Epoch:0, loss:17.251590728759766, Rollout:30
Train -- Epoch:0, loss:17.706819534301758, Rollout:30
Train -- Epoch:0, loss:15.528696060180664, Rollout:30
Train -- Epoch:0, loss:15.535343170166016, Rollout:30
Train -- Epoch:0, loss:18.211345672607422, Rollout:30
Train -- Epoch:0, loss:16.484113693237305, Rollout:30
Train -- Epoch:0, loss:16.416873931884766, Rollout:30
Train -- Epoch:0, loss:20.89557647705078, Rollout:30
Train -- Epoch:0, loss:18.249858856201172, Rollout:30
Train -- Epoch:0, loss:16.597389221191406, Rollout:30
Train -- Epoch:0, loss:16.681974411010742, Rollout:30
Train -- Epoch:0, loss:20.773271560668945, Rollout:30
Train -- Epoch:0, loss:19.108299255371094, Rollout:30
Train -- Epoch:0, loss:17.291309356689453, Rollout:30
Train -- Epoch:0, loss:19.54730987548828, Rollout:30
Train -- Epoch:0, loss:15.415684700012207, Rollout:30
Train -- Epoch:0, loss:15.473921775817871, Rollout:30
Train -- Epoch:0, loss:15.986137390136719, Rollout:30
Train -- Epoch:0, loss:16.585830688476562, Rollout:30
Train -- Epoch:0, loss:16.20871925354004, Rollout:30
Train -- Epoch:0, loss:16.59813690185547, Rollout:30
Train -- Epoch:0, loss:16.570375442504883, Rollout:30
Train -- Epoch:0, loss:16.704030990600586, Rollout:30
Train -- Epoch:0, loss:16.357528686523438, Rollout:30
Train -- Epoch:0, loss:17.405807495117188, Rollout:30
Train -- Epoch:0, loss:16.141138076782227, Rollout:30
Train -- Epoch:0, loss:17.108633041381836, Rollout:30
Train -- Epoch:0, loss:16.659610748291016, Rollout:30
Train -- Epoch:0, loss:15.583953857421875, Rollout:30
Train -- Epoch:0, loss:16.864395141601562, Rollout:30
Train -- Epoch:0, loss:16.819805145263672, Rollout:30
Train -- Epoch:0, loss:17.342918395996094, Rollout:30
Train -- Epoch:0, loss:16.98424530029297, Rollout:30
Train -- Epoch:0, loss:17.016199111938477, Rollout:30
Train -- Epoch:0, loss:15.77621841430664, Rollout:30
Train -- Epoch:0, loss:16.195446014404297, Rollout:30
Train -- Epoch:0, loss:16.097896575927734, Rollout:30
Train -- Epoch:0, loss:18.023239135742188, Rollout:30
Train -- Epoch:0, loss:15.92885971069336, Rollout:30
Train -- Epoch:0, loss:17.250465393066406, Rollout:30
Train -- Epoch:0, loss:15.304193496704102, Rollout:30
Train -- Epoch:0, loss:16.706281661987305, Rollout:30
Train -- Epoch:0, loss:16.940265655517578, Rollout:30
Train -- Epoch:0, loss:16.978008270263672, Rollout:30
Train -- Epoch:0, loss:15.751190185546875, Rollout:30
Train -- Epoch:0, loss:15.17261791229248, Rollout:30
Train -- Epoch:0, loss:16.213237762451172, Rollout:30
Train -- Epoch:0, loss:17.107036590576172, Rollout:30
Train -- Epoch:0, loss:17.05527687072754, Rollout:30
Train -- Epoch:0, loss:16.383272171020508, Rollout:30
Train -- Epoch:0, loss:16.26798439025879, Rollout:30
Train -- Epoch:0, loss:14.839580535888672, Rollout:30
Train -- Epoch:0, loss:16.043760299682617, Rollout:30
Train -- Epoch:0, loss:16.64956283569336, Rollout:30
Train -- Epoch:0, loss:16.66303253173828, Rollout:30
Train -- Epoch:0, loss:16.220354080200195, Rollout:30
Train -- Epoch:0, loss:16.495525360107422, Rollout:30
Train -- Epoch:0, loss:16.18320083618164, Rollout:30
Train -- Epoch:0, loss:17.693378448486328, Rollout:30
Train -- Epoch:0, loss:16.607257843017578, Rollout:30
Train -- Epoch:0, loss:17.129440307617188, Rollout:30
Train -- Epoch:0, loss:17.66905403137207, Rollout:30
Train -- Epoch:0, loss:16.900976181030273, Rollout:30
Train -- Epoch:0, loss:16.450199127197266, Rollout:30
Train -- Epoch:0, loss:16.418874740600586, Rollout:30
Train -- Epoch:0, loss:15.920221328735352, Rollout:30
Train -- Epoch:0, loss:18.180078506469727, Rollout:30
Train -- Epoch:0, loss:16.47406005859375, Rollout:30
Train -- Epoch:0, loss:18.724750518798828, Rollout:30
Train -- Epoch:0, loss:22.768531799316406, Rollout:30
Train -- Epoch:0, loss:17.71729278564453, Rollout:30
Train -- Epoch:0, loss:14.371604919433594, Rollout:30
Train -- Epoch:0, loss:16.86528778076172, Rollout:30
Train -- Epoch:0, loss:18.407821655273438, Rollout:30
Train -- Epoch:0, loss:15.237586975097656, Rollout:30
Train -- Epoch:0, loss:18.22303581237793, Rollout:30
Train -- Epoch:0, loss:17.606164932250977, Rollout:30
Train -- Epoch:0, loss:16.601024627685547, Rollout:30
Train -- Epoch:0, loss:16.680355072021484, Rollout:30
Train -- Epoch:0, loss:18.041662216186523, Rollout:30
Train -- Epoch:0, loss:15.979187965393066, Rollout:30
Train -- Epoch:0, loss:17.419414520263672, Rollout:30
Train -- Epoch:0, loss:17.339736938476562, Rollout:30
Train -- Epoch:0, loss:16.99034881591797, Rollout:30
Train -- Epoch:0, loss:16.431705474853516, Rollout:30
Train -- Epoch:0, loss:17.652780532836914, Rollout:30
Train -- Epoch:0, loss:15.808879852294922, Rollout:30
Train -- Epoch:0, loss:17.122339248657227, Rollout:30
Train -- Epoch:0, loss:16.64615249633789, Rollout:30
Train -- Epoch:0, loss:17.294910430908203, Rollout:30
Train -- Epoch:0, loss:18.36565589904785, Rollout:30
Train -- Epoch:0, loss:18.24135971069336, Rollout:30
Train -- Epoch:0, loss:16.296072006225586, Rollout:30
Train -- Epoch:0, loss:16.30657196044922, Rollout:30
Train -- Epoch:0, loss:17.012104034423828, Rollout:30
Train -- Epoch:0, loss:16.527406692504883, Rollout:30
Train -- Epoch:0, loss:16.02564239501953, Rollout:30
Train -- Epoch:0, loss:16.631174087524414, Rollout:30
Train -- Epoch:0, loss:16.86927604675293, Rollout:30
Train -- Epoch:0, loss:15.843694686889648, Rollout:30
Train -- Epoch:0, loss:16.1345272064209, Rollout:30
Train -- Epoch:0, loss:17.615493774414062, Rollout:30
Train -- Epoch:0, loss:15.979206085205078, Rollout:30
Train -- Epoch:0, loss:16.51919937133789, Rollout:30
Train -- Epoch:0, loss:18.857431411743164, Rollout:30
Train -- Epoch:0, loss:16.348358154296875, Rollout:30
Train -- Epoch:0, loss:17.17928123474121, Rollout:30
Train -- Epoch:0, loss:16.609233856201172, Rollout:30
Train -- Epoch:0, loss:16.103185653686523, Rollout:30
Train -- Epoch:0, loss:16.968151092529297, Rollout:30
Train -- Epoch:0, loss:16.806114196777344, Rollout:30
Train -- Epoch:0, loss:16.203182220458984, Rollout:30
Train -- Epoch:0, loss:16.200458526611328, Rollout:30
Train -- Epoch:0, loss:17.470415115356445, Rollout:30
Train -- Epoch:0, loss:20.16153907775879, Rollout:30
Train -- Epoch:0, loss:17.343887329101562, Rollout:30
Train -- Epoch:0, loss:17.959793090820312, Rollout:30
Train -- Epoch:0, loss:15.945965766906738, Rollout:30
Train -- Epoch:0, loss:17.302608489990234, Rollout:30
Train -- Epoch:0, loss:17.189241409301758, Rollout:30
Train -- Epoch:0, loss:17.340852737426758, Rollout:30
Train -- Epoch:0, loss:16.88719940185547, Rollout:30
Train -- Epoch:0, loss:17.18513298034668, Rollout:30
Train -- Epoch:0, loss:18.60035514831543, Rollout:30
Train -- Epoch:0, loss:17.869245529174805, Rollout:30
Train -- Epoch:0, loss:16.283143997192383, Rollout:30
Train -- Epoch:0, loss:16.316829681396484, Rollout:30
Train -- Epoch:0, loss:15.331155776977539, Rollout:30
Train -- Epoch:0, loss:16.054534912109375, Rollout:30
Train -- Epoch:0, loss:17.04922866821289, Rollout:30
Train -- Epoch:0, loss:16.278709411621094, Rollout:30
Train -- Epoch:0, loss:17.186100006103516, Rollout:30
Train -- Epoch:0, loss:18.928895950317383, Rollout:30
Train -- Epoch:0, loss:15.697624206542969, Rollout:30
Train -- Epoch:0, loss:17.833450317382812, Rollout:30
Train -- Epoch:0, loss:17.426742553710938, Rollout:30
Train -- Epoch:0, loss:17.285533905029297, Rollout:30
Train -- Epoch:0, loss:16.36762237548828, Rollout:30
Train -- Epoch:0, loss:19.64275360107422, Rollout:30
Train -- Epoch:0, loss:17.61252212524414, Rollout:30
Train -- Epoch:0, loss:17.167369842529297, Rollout:30
Train -- Epoch:0, loss:18.062294006347656, Rollout:30
Train -- Epoch:0, loss:16.68708610534668, Rollout:30
Train -- Epoch:0, loss:16.66596794128418, Rollout:30
Train -- Epoch:0, loss:16.648792266845703, Rollout:30
Train -- Epoch:0, loss:16.434221267700195, Rollout:30
Train -- Epoch:0, loss:16.61533546447754, Rollout:30
Train -- Epoch:0, loss:17.066875457763672, Rollout:30
Train -- Epoch:0, loss:17.29480743408203, Rollout:30
Train -- Epoch:0, loss:19.277517318725586, Rollout:30
Train -- Epoch:0, loss:16.774526596069336, Rollout:30
Train -- Epoch:0, loss:17.86832046508789, Rollout:30
Train -- Epoch:0, loss:14.295051574707031, Rollout:30
Train -- Epoch:0, loss:16.836483001708984, Rollout:30
Train -- Epoch:0, loss:17.14314842224121, Rollout:30
Train -- Epoch:0, loss:17.944934844970703, Rollout:30
Train -- Epoch:0, loss:16.601308822631836, Rollout:30
Train -- Epoch:0, loss:18.456100463867188, Rollout:30
Train -- Epoch:0, loss:17.19759178161621, Rollout:30
Train -- Epoch:0, loss:19.004989624023438, Rollout:30
Train -- Epoch:0, loss:16.89937973022461, Rollout:30
Train -- Epoch:0, loss:18.177349090576172, Rollout:30
Train -- Epoch:0, loss:17.334606170654297, Rollout:30
Train -- Epoch:0, loss:17.4252986907959, Rollout:30
Train -- Epoch:0, loss:16.631717681884766, Rollout:30
Train -- Epoch:0, loss:16.166696548461914, Rollout:30
Train -- Epoch:0, loss:16.76479721069336, Rollout:30
Train -- Epoch:0, loss:15.947929382324219, Rollout:30
Train -- Epoch:0, loss:16.91728973388672, Rollout:30
Train -- Epoch:0, loss:17.362979888916016, Rollout:30
Train -- Epoch:0, loss:16.398773193359375, Rollout:30
Train -- Epoch:0, loss:16.467567443847656, Rollout:30
Train -- Epoch:0, loss:16.16652488708496, Rollout:30
Train -- Epoch:0, loss:16.666044235229492, Rollout:30
Train -- Epoch:0, loss:18.282894134521484, Rollout:30
Train -- Epoch:0, loss:19.894142150878906, Rollout:30
Train -- Epoch:0, loss:19.436569213867188, Rollout:30
Train -- Epoch:0, loss:16.639039993286133, Rollout:30
Train -- Epoch:0, loss:17.304410934448242, Rollout:30
Train -- Epoch:0, loss:15.636567115783691, Rollout:30
Train -- Epoch:0, loss:16.73729705810547, Rollout:30
Train -- Epoch:0, loss:16.474449157714844, Rollout:30
Train -- Epoch:0, loss:18.383485794067383, Rollout:30
Train -- Epoch:0, loss:16.431167602539062, Rollout:30
Train -- Epoch:0, loss:15.836202621459961, Rollout:30
Train -- Epoch:0, loss:16.079967498779297, Rollout:30
Train -- Epoch:0, loss:16.756982803344727, Rollout:30
Train -- Epoch:0, loss:17.682626724243164, Rollout:30
Train -- Epoch:0, loss:16.25938606262207, Rollout:30
Train -- Epoch:0, loss:17.263853073120117, Rollout:30
Train -- Epoch:0, loss:17.03073501586914, Rollout:30
Train -- Epoch:0, loss:16.946720123291016, Rollout:30
Train -- Epoch:0, loss:18.175403594970703, Rollout:30
Train -- Epoch:0, loss:20.0972843170166, Rollout:30
Train -- Epoch:0, loss:15.948869705200195, Rollout:30
Train -- Epoch:0, loss:17.69720458984375, Rollout:30
Train -- Epoch:0, loss:18.39069366455078, Rollout:30
Train -- Epoch:0, loss:16.288049697875977, Rollout:30
Train -- Epoch:0, loss:16.714786529541016, Rollout:30
Training epoch completed in 169.53139472405115 mins, Total time: 169.53278735876083 mins
Train -- Epoch:1, loss:18.029836654663086, Rollout:30
Train -- Epoch:1, loss:17.17624282836914, Rollout:30
Train -- Epoch:1, loss:16.75747299194336, Rollout:30
Train -- Epoch:1, loss:17.82537841796875, Rollout:30
Train -- Epoch:1, loss:20.895597457885742, Rollout:30
Train -- Epoch:1, loss:16.004993438720703, Rollout:30
Train -- Epoch:1, loss:17.13132667541504, Rollout:30
Train -- Epoch:1, loss:15.828573226928711, Rollout:30
Train -- Epoch:1, loss:17.080474853515625, Rollout:30
Train -- Epoch:1, loss:16.972095489501953, Rollout:30
Train -- Epoch:1, loss:18.29567527770996, Rollout:30
Train -- Epoch:1, loss:16.453107833862305, Rollout:30
Train -- Epoch:1, loss:15.893489837646484, Rollout:30
Train -- Epoch:1, loss:15.915281295776367, Rollout:30
Train -- Epoch:1, loss:16.524038314819336, Rollout:30
Train -- Epoch:1, loss:17.929882049560547, Rollout:30
Train -- Epoch:1, loss:14.644485473632812, Rollout:30
Train -- Epoch:1, loss:16.12062644958496, Rollout:30
Train -- Epoch:1, loss:17.130783081054688, Rollout:30
Train -- Epoch:1, loss:16.32358741760254, Rollout:30
Train -- Epoch:1, loss:15.043571472167969, Rollout:30
Train -- Epoch:1, loss:15.886374473571777, Rollout:30
Train -- Epoch:1, loss:16.500810623168945, Rollout:30
Train -- Epoch:1, loss:17.139432907104492, Rollout:30
Train -- Epoch:1, loss:16.586977005004883, Rollout:30
Train -- Epoch:1, loss:15.63346004486084, Rollout:30
Train -- Epoch:1, loss:16.64900016784668, Rollout:30
Train -- Epoch:1, loss:16.737979888916016, Rollout:30
Train -- Epoch:1, loss:17.771291732788086, Rollout:30
Train -- Epoch:1, loss:16.61809539794922, Rollout:30
Train -- Epoch:1, loss:16.857383728027344, Rollout:30
Train -- Epoch:1, loss:16.013198852539062, Rollout:30
Train -- Epoch:1, loss:16.212066650390625, Rollout:30
Train -- Epoch:1, loss:18.389209747314453, Rollout:30
Train -- Epoch:1, loss:18.072542190551758, Rollout:30
Train -- Epoch:1, loss:15.82332992553711, Rollout:30
Train -- Epoch:1, loss:16.5875186920166, Rollout:30
Train -- Epoch:1, loss:14.621659278869629, Rollout:30
Train -- Epoch:1, loss:16.548795700073242, Rollout:30
Train -- Epoch:1, loss:15.946721076965332, Rollout:30
Train -- Epoch:1, loss:13.901283264160156, Rollout:30
Train -- Epoch:1, loss:16.938579559326172, Rollout:30
Train -- Epoch:1, loss:17.9798583984375, Rollout:30
Train -- Epoch:1, loss:16.194271087646484, Rollout:30
Train -- Epoch:1, loss:17.13357162475586, Rollout:30
Train -- Epoch:1, loss:16.503890991210938, Rollout:30
Train -- Epoch:1, loss:16.9338321685791, Rollout:30
Train -- Epoch:1, loss:14.93954849243164, Rollout:30
Train -- Epoch:1, loss:18.04960060119629, Rollout:30
Train -- Epoch:1, loss:16.464313507080078, Rollout:30
Train -- Epoch:1, loss:16.79029083251953, Rollout:30
Train -- Epoch:1, loss:15.047140121459961, Rollout:30
Train -- Epoch:1, loss:16.80422592163086, Rollout:30
Train -- Epoch:1, loss:15.948415756225586, Rollout:30
Train -- Epoch:1, loss:15.733755111694336, Rollout:30
Train -- Epoch:1, loss:16.400623321533203, Rollout:30
Train -- Epoch:1, loss:17.89189338684082, Rollout:30
Train -- Epoch:1, loss:18.145267486572266, Rollout:30
Train -- Epoch:1, loss:16.368871688842773, Rollout:30
Train -- Epoch:1, loss:17.36893653869629, Rollout:30
Train -- Epoch:1, loss:16.43355369567871, Rollout:30
Train -- Epoch:1, loss:18.528270721435547, Rollout:30
Train -- Epoch:1, loss:15.816136360168457, Rollout:30
Train -- Epoch:1, loss:17.116226196289062, Rollout:30
Train -- Epoch:1, loss:16.214439392089844, Rollout:30
Train -- Epoch:1, loss:16.660076141357422, Rollout:30
Train -- Epoch:1, loss:16.42206573486328, Rollout:30
Train -- Epoch:1, loss:16.423067092895508, Rollout:30
Train -- Epoch:1, loss:15.367439270019531, Rollout:30
Train -- Epoch:1, loss:18.227760314941406, Rollout:30
Train -- Epoch:1, loss:17.25847053527832, Rollout:30
Train -- Epoch:1, loss:15.043734550476074, Rollout:30
Train -- Epoch:1, loss:18.2786865234375, Rollout:30
Train -- Epoch:1, loss:17.756208419799805, Rollout:30
Train -- Epoch:1, loss:15.769271850585938, Rollout:30
Train -- Epoch:1, loss:16.25802230834961, Rollout:30
Train -- Epoch:1, loss:17.3897705078125, Rollout:30
Train -- Epoch:1, loss:18.194660186767578, Rollout:30
Train -- Epoch:1, loss:17.115371704101562, Rollout:30
Traceback (most recent call last):
  File "lstm_prog_train_test.py", line 1080, in <module>
    if __name__ == "__main__":
  File "lstm_prog_train_test.py", line 1005, in main
    30, # disable rollout_length curriculum
  File "lstm_prog_train_test.py", line 443, in train
    loss.backward()
  File "/home/jasonyma/anaconda3/lib/python3.7/site-packages/torch/tensor.py", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jasonyma/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py", line 127, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt
